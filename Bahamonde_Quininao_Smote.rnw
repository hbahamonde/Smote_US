\documentclass[onesided]{article}
\usepackage[T1]{fontenc}
\linespread{2} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[hmarginratio=1:1,columnsep=20pt]{geometry} % Document margins
%\usepackage{multicol} % Used for the two-column layout of the document
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{float} % Required for tables and figures in the multi-column environment - they need to be placed in specific locations with the [H] (e.g. \begin{table}[H])

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text
\usepackage{paralist} % Used for the compactitem environment which makes bullet points with less space between them

% to ignore texts: good for thank messages and paper submissions.
      % \fbox{\phantom{This text will be invisible too, but a box will be printed arround it.}}

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
%\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage[]{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\Roman{subsection}} % Roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancybox, fancyvrb, calc}
\usepackage[svgnames]{xcolor}
\usepackage{epigraph}
\usepackage{longtable}
\usepackage{pdflscape}
\usepackage{graphics}
\usepackage{pbox} % \pbox{20cm}{This is the first \\ cell}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{rotating}
\usepackage{paracol}
\usepackage{textcomp}
\usepackage[export]{adjustbox}
\usepackage{afterpage}
\usepackage{filecontents}
\usepackage{color}
\usepackage{latexsym}
\usepackage{lscape}       %\begin{landscape} and \end{landscape}
\usepackage{wasysym}
\usepackage{dashrule}

\usepackage{framed}
\usepackage{tree-dvips}
\usepackage{pgffor}
\usepackage[]{authblk}
\usepackage{setspace}
\usepackage{array}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}     %desactivar para link rojos
\usepackage{graphicx}
\usepackage{dcolumn} % for R tables
\usepackage{multirow} % For multirow in tables
\usepackage{pifont}
\usepackage{listings}



% hypothesis / theorem package begin
\usepackage{amsthm}
\usepackage{thmtools}
\declaretheoremstyle[
spaceabove=6pt, spacebelow=6pt,
headfont=\normalfont\bfseries,
notefont=\mdseries, notebraces={(}{)},
bodyfont=\normalfont,
postheadspace=0.6em,
headpunct=:
]{mystyle}
\declaretheorem[style=mystyle, name=Hypothesis, preheadhook={\renewcommand{\thehyp}{H\textsubscript{\arabic{hyp}}}}]{hyp}

\usepackage{cleveref}
\crefname{hyp}{hypothesis}{hypotheses}
\Crefname{hyp}{Hypothesis}{Hypotheses}
% hypothesis / theorem package end


%----------------------------------------------------------------------------------------
% Other ADDS-ON
%----------------------------------------------------------------------------------------

% independence symbol \independent
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}



% Les principaux ensembles
\newcommand{\Abs}[1]{\left\lvert#1\right\rvert}
\newcommand\N{{\mathbb N}}
\newcommand\R{{\mathbb R}}
\newcommand\T{{\mathbb T}}
\newcommand\C{{\mathbb C}}
\newcommand\Q{{\mathbb Q}}
\newcommand\Z{{\mathbb Z}}
\newcommand\Pp{{\mathbb P}}
\newcommand\Ee{{\mathbb E}}
\def\x{{\mathbf x}}
\def\w{{\mathbf w}}
\def\xxi{{\pmb \xi}}




\hypersetup{
    bookmarks=true,         % show bookmarks bar?
    unicode=false,          % non-Latin characters in Acrobat's bookmarks
    pdftoolbar=true,        % show Acrobat's toolbar?
    pdfmenubar=true,        % show Acrobat's menu?
    pdffitwindow=true,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={My title},    % title
    pdfauthor={Author},     % author
    pdfsubject={Subject},   % subject of the document
    pdfcreator={Creator},   % creator of the document
    pdfproducer={Producer}, % producer of the document
    pdfkeywords={keyword1} {key2} {key3}, % list of keywords
    pdfnewwindow=true,      % links in new window
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=Maroon,          % color of internal links (change box color with linkbordercolor)
    citecolor=Maroon,        % color of links to bibliography
    filecolor=Maroon,      % color of file links
    urlcolor=Maroon           % color of external links
}

%\usepackage[nodayofweek,level]{datetime} % to have date within text

\newcommand{\LETT}[3][]{\lettrine[lines=4,loversize=.2,#1]{\smash{#2}}{#3}} % letrine customization



% comments on margin
  % Select what to do with todonotes: 
  % \usepackage[disable]{todonotes} % notes not showed
  \usepackage[draft]{todonotes}   % notes showed
  % usage: \todo{This is a note at margin}

\usepackage{cooltooltips}

%%% bib begin
\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage[backend=biber,style=authoryear,dashed=false,doi=false,isbn=false,url=false,arxiv=false]{biblatex}
%\DeclareLanguageMapping{american}{american-apa}
\addbibresource{Bahamonde_Quininao_Smote.bib} 


% USAGES
%% use \textcite to cite normal
%% \parencite to cite in parentheses
%% \footcite to cite in footnote
%% the default can be modified in autocite=FOO, footnote, for ex. 
%%% bib end


% r code verbatim config
\lstdefinestyle{R}{ %
  language=R,                     % the language of the code
  basicstyle=\footnotesize,       % the size of the fonts that are used for the code
  numbers=left,                   % where to put the line-numbers
  numberstyle=\tiny\color{gray},  % the style that is used for the line-numbers
  stepnumber=1,                   % the step between two line-numbers. If it's 1, each line
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=single,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  title=\lstname,                 % show the filename of files included with \lstinputlisting;
                                  % also try caption instead of title
  keywordstyle=\color{blue},      % keyword style
  commentstyle=\color{dkgreen},   % comment style
  stringstyle=\color{mauve},      % string literal style
  morekeywords={*,...}            % if you want to add more keywords to the set
}

\lstdefinestyle{Python}{ %
  language=Python,                     % the language of the code
  basicstyle=\footnotesize,       % the size of the fonts that are used for the code
  numbers=left,                   % where to put the line-numbers
  numberstyle=\tiny\color{gray},  % the style that is used for the line-numbers
  stepnumber=1,                   % the step between two line-numbers. If it's 1, each line
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=single,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  title=\lstname,                 % show the filename of files included with \lstinputlisting;
                                  % also try caption instead of title
  keywordstyle=\color{blue},      % keyword style
  commentstyle=\color{dkgreen},   % comment style
  stringstyle=\color{mauve},      % string literal style
  morekeywords={*,...}            % if you want to add more keywords to the set
} 

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}


\lstset{language=R,frame=lines}
\lstset{language=Python,frame=lines}



% DOCUMENT ID



% TITLE SECTION

\title{\vspace{-15mm}\fontsize{18pt}{7pt}\selectfont\textbf{\input{title.txt}\unskip}} % Article title


\author[1]{

\textsc{H\'ector Bahamonde}
\thanks{\href{mailto:hector.bahamonde@uoh.cl}{hector.bahamonde@uoh.cl}; \href{http://www.hectorbahamonde.com}{\texttt{www.HectorBahamonde.com}}.}}



\author[2]{

\textsc{Cristobal Quininao}
\thanks{\href{mailto:cristobal.quininao@uoh.cl}{cristobal.quininao@uoh.cl}; 
\href{https://cquininao.wordpress.com}{\texttt{https://cquininao.wordpress.com}}. \\
Authors are listed in alphabetical order. This project was funded by the Center for \fbox{\phantom{the Experimental Study of Psychology and Politics}} at \fbox{\phantom{Rutgers University---New Brunswick}}.}}


\affil[1]{Assistant Professor, Instituto de Ciencias Sociales, O$'$Higgins University}
\affil[2]{Assistant Professor, Instituto de Ciencias de la Ingenier\'ia O$'$Higgins University}


\date{\today}

%----------------------------------------------------------------------------------------

\begin{document}
%\SweaveOpts{concordance=TRUE}
% Sweave2knitr("Bahamonde_Quininao_Smote_US.rnw")
\pagenumbering{gobble} 


\setcounter{hyp}{0} % sets hypothesis counter to 1

\maketitle % Insert title


%----------------------------------------------------------------------------------------
% ABSTRACT
%----------------------------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% loading knitr package

<<echo=FALSE, cache=FALSE, warning = FALSE, message = F>>=
read_chunk('/Users/hectorbahamonde/research/Smote_US/Bahamonde_Quininao_Smote_US.R') # Hector path // MAC
#Â read_chunk('Bahamonde_Quininao_Smote_US.R') % Cristobal path

@


<<rsetup, include=FALSE>>=
chooseCRANmirror(graphics=FALSE, ind=1)
if (!require("pacman")) install.packages("pacman"); library(pacman)
p_load(knitr)
options(scipen = 99999999999)

@

<<abstract, echo=FALSE, comment=NA, warning=FALSE, message=FALSE, include = FALSE, cache=FALSE, eval=TRUE>>=

@

<<abstract.length, echo=FALSE, comment=NA, warning=FALSE, message=FALSE, include = FALSE, cache=FALSE, eval=TRUE>>=

@

% end knitr stuff
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\begin{abstract}
\input{abstract.txt}\unskip
\end{abstract}

\hspace*{5cm}{{\bf Abstract length}: \Sexpr{abstract.c.l} words}.

\vspace*{1cm}

\hspace*{1.3cm}{\bf Please consider downloading the last version of the paper} \href{https://github.com/hbahamonde/Smote_US/raw/main/Bahamonde_Quininao_Smote.pdf}{\texttt{{\color{red}here}}}.

\vspace*{1cm}

\providecommand{\keywords}[1]{\textbf{\emph{Keywords---}} #1} % keywords.  
\keywords{United States; machine learning; SMOTE; ideology.}
\clearpage



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONTENT (write the paper below)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\newpage
\pagenumbering{arabic}
\setcounter{page}{1}

\linespread{2}

\section{Ideology}



\section{Machine Learning in Political Science}

``inductive learning'' Cranmer2019, p. 1

``we use ML for prediction'' Cranmer2019, p. 1

``ML algorithms can usually predict outcomes with greater accuracy than the standard regression-type models we use in political science (often \emph{much} greater accuracy)''\footnote{Emphasis in original.} Cranmer2019, p. 1

``95 percent and 100 percent accuracy when used to construct a classification tree using the entire data set.'' Schrodt1990, p 52

``introduce a novel method to diagnose electoral irregularities using vote counts. Specifically, we propose the use of machine learning techniques to detect fraud'' Cantu2011a p 401

\section{Data}



\section{Materials and Methods}

The overall model training and validation process consists in a three phases. The data were normalized employing the MinMax scaler\todo{citar: que es, de donde sale, que hace}. Exploiting the normalized dataset, two different experiments were performed. In the first experiment machine learning algorithms were executed to classify survey participants according to their ideological preferences in a liberal-conservative scale\todo{hb: de cuanto era la escala}. For classification purposes, the ideological preferences of survey participants were recoded in weather they were liberals or conservatives\todo{hb: es un estudio es solo liberal}. Since the resulting training pairs datasets are strongly imbalanced\todo{considerar una pequena motivacion de si esto es un problema o ventaja, o que significa}, prior to the construction of the learning models implemented in this paper, the SMOTE oversampling procedure was used \parencite{chawla2002smote}. Five nearest neighbors for each vote-selling sample were employed \parencite{artetxe2020balanced}. The resulting training data pairs were then validated via a repeated cross-validation process (RCV). The RCV process consists of constructing several machine learning models by using the same dataset but under the idea of generating folds and repetition\todo{citar}. In particular, each cross-validation repetition consists of partitioning the dataset in a particular number of folds, using each fold as a test dataset while using all the remaining data for model training. Finally, average performance measures across all repetitions were reported. In this manuscript, the reported results are the average of the 30 repetitions of the RCV models.

\subsection{Classification Methods}

Within the predictive model framework, several machine learning approaches can be used for analyzing the data \parencite{vapnik2013nature,witten2002data,maimon2005data}. Due to the small dataset used in these experiments, applying deep learning algorithms were discarded \parencite{balas2019handbook}. Therefore, we focus in the following classical well-known methods: \textit{Support Vector Machines}, \textit{Multilayer Perceptron} and \textit{Na\"ive Bayes method}.

\paragraph{Support Vector Machines (SVM)} Support vector machines (SVMs) are a set of related methods for supervised learning which have applications to classification (the output data is a sequence of tags) and regression (the output data is a continuous variable) problems \parencite[Ch. 12]{maimon2005data}. When using SVM for classifying purposes, the goal is finding linear functions that are able to separate the data based on the support vectors---or ``boundaries''---of the different classes. The idea of the algorithm is to find the optimal hyperplane, i.e., the hyperplane that maximizes the distance between the boundaries of each class. This can be done by implementing a quadratic programming problem \parencite{vapnik2013nature}. When the classes are not linearly separable, then it is possible to place the data into a space of higher dimensionality using a kernel transformation \parencite{vapnik2013nature,maimon2005data} so that the transformed dataset become linearly separable. We used the SVM library \emph{Sklearn} implemented in \texttt{Python} for training and estimation purposes. Both the linear and Radial Bases Functions (RBF) were implemented.

\paragraph{Multilayer Perceptron} Multilayer perceptron (MLP) is the classical feed-forward artificial neural networks (ANN) composed by multiple densely interconnected layers of computational units, usually known as ``artificial neurons'' \parencite{wolff2019machine}. The method corresponds to the family of supervised learning algorithms that find a function able to map multidimensional samples. In this application the focus is on mapping the socio-demographic battery of questions administered to survey participants into the vote-selling question. MLPs are constructed as follows: {\color{red}(1) a number of hidden layers is fixed \todo{meaning de hidden? fixed?}, (2) the architecture \todo{architecture?} starts by defining $m$ artificial neurons---i.e. the set of features of the input data corresponding to the input layer, (3) any artificial neuron in the first hidden layer integrates the inputs from the input layer and combines the input values with a weighted linear summation (the weights become part of the parameters to be tuned in the learning phase), (iv) the result of this summation is nonlinearly transformed through an activation function (for instance an hyperbolic $\tanh$ function). The procedure is repeated for each hidden layer, until the output layer is reached}\todo{me perdi a cagar!}. The connection weights can be learned from data applying the back-propagation algorithm \parencite{haykin2004comprehensive}. The training and estimation processes were executed via the \emph{MLPClassifier} module implemented in the \emph{Sklearn} \texttt{Python} library. The \emph{MLPClassifier} implements a MLP algorithm that trains using back-propagation through a stochastic gradient descent. The rectified linear unit function for the activation \todo{activation?} of the hidden layers was used. The MLP model constructed has two hidden layers, each of 50 neurons. 

\paragraph{Na\"ive Bayes Method} Na\"ive Bayes methods are a set of supervised learning algorithms that apply Bayes' theorem with the ``naive'' assumption that all features of an individual are independent. Bayes' theorem provides a rule to calculate the conditional probability of an event given some prior knowledge. This method allows estimating the joint probability distribution of a feature vector as the product of the unidimensional distribution probabilities of each feature \parencite{wolff2019machine}. In this application the \emph{GaussianNB} module from \emph{Sklearn} \texttt{Python} library implements the Gaussian Naive Bayes algorithm. The likelihood of the features is assumed to be Gaussian.

\subsection{Classification Performance Metrics} We illustrate the chosen sensitivity metrics with the liberal classification machines only---the conservative situation is identical.\footnote{Tables available in the Appendix section.} At the end of each cross-validation fold, two different vectors are produced: $y_{test}$ corresponding to the actual responses given by survey participants to the liberal question (restricted to the fold used for testing), and $y_{pred}$ corresponding to the predicted answers given by the model for the input features. With these two vectors at hand it is possible to compute the following quantities:

\begin{itemize}
\item True positive counts (TP): number of survey participants such that the model predicts as very liberal and they actually belong to the very liberal class.
\item True negative counts (TN): number of survey participants such that the model predicts as not very liberal and they do not belong to the very liberal class.
\item False positive counts (FP): number of survey participants such that the model predicts as very liberal but they do not belong to the very liberal class.
\item False negative counts (FN): number of survey participants such that the model predicts as not very liberal but they actually belong to the very liberal class.
\end{itemize}

With these numbers, the recall (\emph{R})---or model sensitivity---was computed.\footnote{The fraction of examples classified as very liberal, among the total number of very liberal examples.} Particularly, the positive predictive value (\emph{PPV}) or ``precision''\footnote{The fraction of true very liberal examples among the examples that the model classified as very liberal.} as well as the \emph{f}-score were computed considering the following relationships,\footnote{A perfect model has an $f$-score of 1.}

\begin{equation}\label{eq:1}
R=\frac{TP}{TP+FN},\quad PPV=\frac{TP}{TP+FP},\quad f=\frac{2}{\frac{1}{R}+\frac{1}{PPV}}.
\end{equation}

The average and the standard deviation across all cross-validation folds are also reported in \autoref{tab:f_auc}. 

An alternative metric is the accuracy metric (\emph{A}) defined as,

\begin{equation}\label{eq:2}
A = \frac{TP+TN}{TP+TN+FP+FN}=\frac{TP+TN}{\text{size of the fold}}.
\end{equation}

However, considering that the data are strongly imbalanced, metrics defined in \autoref{eq:1} and \autoref{eq:2} are more informative. Another metric widely used to compare binary classifiers is the Receiver Operating Curve (\emph{ROC}) \parencite{wolff2019machine,artetxe2020balanced} which plots sensitivity \emph{R} defined in \autoref{eq:1} against the false positive rate (FPR) defined as follows,

\begin{equation}
FPR = \frac{FP}{FP+TN}.
\end{equation}

The ROC metric is computed as a probability curve, and the area under the ROC curve represents the degree of separability. In other words the ROC quantifies how much the model is capable of distinguishing between classes. In this application the higher the area under the curve, the better the model is at predicting liberals as liberals and non liberals as non liberals. Remark that (1) a perfect model has an area under curve of 1, (2) a model with an area under the curve of 0 implies that the model is actually reciprocating the classes, therefore simply relabeling is enough to get good results; and (3) the worst-case scenario is when the area under the curve equals 0.5, meaning that the model has no class separation capacity.

To actually compute these metrics every fold test is compared \todo{compared, ta bn?} against the remaining folds as training dataset. Since the resulting training data set is imbalanced, the SMOTE oversampling technique with five neighbors on the minority class is used. \todo{falta explicar brevemente que hace y de donde sale el smote} Remark that the test set by design \todo{es by design?} remains imbalanced which has some consequences. In particular, even one misclassified sample might translate into large reductions of the performance measures. Moreover, the number of remaining samples of liberal-leaning survey respondents in each fold depends on the number of folds, thus having consequences in the imbalance ratio. We report results with decreasing number of RVC \todo{RVC??} folds in order to test the stability of the results.

\section{Results}

\autoref{tab:r_ppv} and \autoref{tab:f_auc} show the results of all metrics for all the mentioned machine learning techniques after 40 repetitions (3 fold\todo{se dice en singular?}), 10 repetitions (4 fold), 8 repetitions (5 fold) and 4 repetitions (10 fold) of the repeated cross-validation process (RCV) experiments with the SMOTE class imbalance correction. There is no statistical evidence to expect that the number of folds has an effect on the performance of each method. An F-test over the number of folds shows that there is no statistically significant differences ($p\gg0.1$).

The PPV, f-Score and AUC metrics suggest that the different machine learning techniques used perform differently. Particularly, SVM with the radial bases functions (RBF) kernel is above \todo{above es bueno o malo?} SVM Linear, MLP, and NB independently of the number of folds. On the other hand, the recall metric $R$ (left panel of \autoref{tab:r_ppv}) shows no sufficient evidence to affirm that methods perform differently \todo{``differently'' meaning ``better''?}. Indeed, calculating an F-test for all machines for all number of folds suggests no strong statistical evidence \todo{evidence of what}, implying that the overall performance improves also considering the $\alpha$ level and the number of folds---for instance, for 10 folds the p-value is approximately $0.026$. However, the results change dramatically when the MLP model is discarded. This model performs below \todo{below what?} any other model ($p\approx0.77$ for 10 folds). We conclude that according to the recall metric, all methods---except for the MLPm method---have similar performance. Figure 2 \todo{add Fig2} shows the ROC curves for all approaches in the case of repeated cross-validation process (RCV) with 5 folders.

The f-scores shown in \autoref{tab:f_auc} confirm that SVM with radial bases functions (RBF) kernel improves over SVM Linear, MLP and NB regardless of repeated cross-validation process (RCV) number of folders. An F-test carried out over these results confirms that the performance differences between predictive models are statistically significant  ($p\approx0.002$) \todo{implying q es bueno o malo?}. Specific one-sided t-tests comparing each pair of modeling approaches confirms that SVM with radial bases functions (RBF) kernel perform better than SVM Linear, MLP and NB. However, the superiority of SVM with RBF kernel relative to MLP is less pronounced ($p\approx0.05$). On the contrary, the effect is more pronounced for the AUC metric. The f-test carried out over these results confirms \todo{confirma que?} ($p\ll0.001$) t-test pairwise tests ($p<0.001$) \todo{redaccion?}. 

\begin{table}[!htbp]
\centering
\footnotesize
\caption{Average $\pm$ standard deviation Recall R (left) and positive predictive value PPV (right) performance of SVM (linear), SVM (RBF), MLP and NB for decreasing number of folders in the repeated cross validation process. All results are calculated with SMOTE oversampling correction of class imbalance.}
\label{tab:r_ppv}
\begin{tabular}{*5c}
\toprule
  \multicolumn{5}{c}{SMOTE} \\
nfolds & SVM (linear) & SVM (RBF) & MLP & NB
\\
\midrule
10 & 0.68$\pm$0.07 & 0.67$\pm$0.08 & 0.65$\pm$0.07 & 0.68$\pm$0.08
\\
5 & 0.68$\pm$0.05 & 0.67$\pm$0.04 & 0.65$\pm$0.06 & 0.66$\pm$0.06
\\
4 & 0.67$\pm$0.03 & 0.66$\pm$0.04 & 0.64$\pm$0.04 & 0.67$\pm$0.04
\\
3 & 0.68$\pm$0.03 & 0.67$\pm$0.05 & 0.67$\pm$0.05 & 0.67$\pm$0.05
\\
\bottomrule
\end{tabular}
\quad
\begin{tabular}{*5c}
\toprule
  \multicolumn{5}{c}{SMOTE} \\
nfolds & SVM (linear) & SVM (RBF) & MLP & NB
\\
\midrule
10 & 0.45$\pm$0.06 & 0.52$\pm$0.08 & 0.47$\pm$0.07 & 0.44$\pm$0.07
\\
5 & 0.46$\pm$0.05 & 0.52$\pm$0.07 & 0.48$\pm$0.05 & 0.44$\pm$0.05
\\
4 & 0.45$\pm$0.04 & 0.52$\pm$0.06 & 0.47$\pm$0.04 & 0.43$\pm$0.05
\\
3 & 0.45$\pm$0.03 & 0.52$\pm$0.04 & 0.45$\pm$0.03 & 0.44$\pm$0.04
\\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[!htbp]
\centering
\footnotesize
\caption{Average $\pm$ Standard Deviation f-score (left) and AUC (right) Performance of SVM (linear), SVM (RBF), MLP and NB for decreasing number of folders in the repeated cross validation process. All results are calculated with SMOTE oversampling correction of class imbalance.}
\label{tab:f_auc}
\begin{tabular}{*5c}
\toprule
  \multicolumn{5}{c}{SMOTE} \\
nfolds & SVM (linear) & SVM (RBF) & MLP & NB
\\
\midrule
10 & 0.54$\pm$0.06 & 0.58$\pm$0.07 & 0.54$\pm$0.06 & 0.53$\pm$0.06
\\
5 & 0.55$\pm$0.05 & 0.59$\pm$0.05 & 0.55$\pm$0.05 & 0.53$\pm$0.05
\\
4 & 0.54$\pm$0.03 & 0.58$\pm$0.04 & 0.54$\pm$0.03 & 0.52$\pm$0.04
\\
3 & 0.54$\pm$0.02 & 0.58$\pm$0.03 & 0.54$\pm$0.03 & 0.53$\pm$0.04
\\
\bottomrule
\end{tabular}
\quad
\begin{tabular}{*5c}
\toprule
  \multicolumn{5}{c}{SMOTE} \\
nfolds & SVM (linear) & SVM (RBF) & MLP & NB
\\
\midrule
10 & 0.68$\pm$0.04 & 0.72$\pm$0.05 & 0.68$\pm$0.04 & 0.67$\pm$0.04
\\
5 & 0.67$\pm$0.03 & 0.72$\pm$0.03 & 0.68$\pm$0.03 & 0.67$\pm$0.03
\\
4 & 0.67$\pm$0.02 & 0.72$\pm$0.04 & 0.67$\pm$0.03 & 0.67$\pm$0.03
\\
3 & 0.68$\pm$0.02 & 0.71$\pm$0.02 & 0.68$\pm$0.02 & 0.67$\pm$0.02
\\
\bottomrule
\end{tabular}

\end{table}


% References
\newpage
\pagenumbering{Roman}
\setcounter{page}{1}
\printbibliography





% EndNotes
%\newpage
%\pagenumbering{Roman}
%\setcounter{page}{1}
%\linespread{2} % 1.5, Line spacing - Palatino needs more space between lines
%\theendnotes
%\linespread{1.5} % 1.5, Line spacing - Palatino needs more space between lines




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% WORD COUNT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage

<<wordcount, echo=FALSE, cache=FALSE, warning=FALSE>>=
library(knitr)

comma <- function (x, ...) {
  format(x, ..., big.mark = ",", scientific = FALSE, trim = TRUE)
}

# To dynamically extract name of the current file, use code below
nameoffile <- current_input() # get name of file
nof2 <-  strsplit(nameoffile,"\\.")[[1]][1] # extract name, drop extension
noftex <- paste(nof2, ".tex", sep="") # add .tex extension
systemcall <- paste("system('texcount -inc -incbib -total -sum ", noftex, "', intern=TRUE)", sep="") # paste together texcount system command
texcount.out <- eval(parse(text=systemcall)) # run texcount on current last compiled .tex file

sum.row <- grep("Sum count", texcount.out, value=TRUE) # extract row
pattern <- "(\\d)+" # regex pattern for digits

count <- regmatches(sum.row, regexpr(pattern, sum.row) )
# extract digits

count <- comma(as.numeric(count)) # add comma
@


\begin{center}
\vspace*{\stretch{1}}
\dotfill
\dotfill {\huge {\bf Word count}: \Sexpr{count}} \dotfill
\dotfill
\vspace*{\stretch{1}}
\end{center}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% WORD COUNT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





% reset counter for appendix
%% reset tables and figures counter
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}

\newpage
\pagenumbering{Roman}
\setcounter{page}{1}

\section{Appendix}\hypertarget{appendix}{}



\subsection{Appendix 1}



% Notes
\linespread{2}
%\newpage\theendnotes


\end{document}

